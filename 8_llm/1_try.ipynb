{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cb0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0054c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentencepiece(corpus_path: str, model_prefix: str, vocab_size: int = 8000, model_type: str = \"bpe\"):\n",
    "    \"\"\"\n",
    "    Train sentencepiece tokenizer from raw corpus.\n",
    "    \"\"\"\n",
    "    cmd = f\"--input={corpus_path} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type={model_type} --character_coverage=1.0 --bos_id=-1 --eos_id=-1\"\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    print(\"Tokenizer saved as:\", model_prefix + \".model\")\n",
    "\n",
    "def load_tokenizer(model_file: str):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_file)\n",
    "    return sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44029d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWindowTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tokenized corpus dataset: random windows of tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_array: np.ndarray, seq_len: int):\n",
    "        self.tokens = token_array\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.tokens) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.tokens) <= self.seq_len + 1:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = random.randint(0, len(self.tokens) - self.seq_len - 1)\n",
    "        x = self.tokens[start:start + self.seq_len].astype(np.int64)\n",
    "        y = self.tokens[start + 1:start + 1 + self.seq_len].astype(np.int64)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac636cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size=128, n_layer=4, n_head=4, n_embd=256, dropout=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).view(1,1,T,T)\n",
    "        att = att.masked_fill(mask==0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        out = att @ v\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        out = self.out_proj(out)\n",
    "        return self.resid_dropout(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.fc2 = nn.Linear(4*n_embd, n_embd)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.fc2(self.act(self.fc1(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config.n_embd, config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok = self.tok_emb(idx)\n",
    "        pos = self.pos_emb[:, :T, :]\n",
    "        x = self.drop(tok + pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d8b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            B,T,V = logits.size()\n",
    "            loss = F.cross_entropy(logits.view(B*T,V), y.view(B*T))\n",
    "            losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=40, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.EncodeAsIds(prompt)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long, device=device)[None,:]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = input_ids[:, -model.config.block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, top_k, dim=-1)\n",
    "            probs = F.softmax(v, dim=-1)\n",
    "            next_id = ix[0, torch.multinomial(probs[0], 1)]\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs[0], 1)\n",
    "        input_ids = torch.cat([input_ids, next_id.view(1,1)], dim=1)\n",
    "    return tokenizer.DecodeIds(input_ids[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe7e02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved as: spm.model\n"
     ]
    }
   ],
   "source": [
    "# Change this to your dataset file path\n",
    "corpus_file = \"wizard_of_oz.txt\"  \n",
    "\n",
    "# Train tokenizer\n",
    "if not os.path.exists(\"spm.model\"):\n",
    "    train_sentencepiece(corpus_file, \"spm\", vocab_size=8000)\n",
    "    \n",
    "tokenizer = load_tokenizer(\"spm.model\")\n",
    "\n",
    "# Load corpus into tokens\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "token_ids = np.array(tokenizer.EncodeAsIds(raw), dtype=np.int32)\n",
    "\n",
    "# Split train/val\n",
    "split = int(len(token_ids)*0.95)\n",
    "train_ids, val_ids = token_ids[:split], token_ids[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9243fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "batch_size = 8\n",
    "\n",
    "train_ds = RandomWindowTextDataset(train_ids, seq_len=block_size)\n",
    "val_ds   = RandomWindowTextDataset(val_ids, seq_len=block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb6955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "cfg = GPTConfig(vocab_size=tokenizer.GetPieceSize(), block_size=block_size,\n",
    "                n_layer=4, n_head=4, n_embd=256)\n",
    "\n",
    "model = TinyGPT(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f61c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Train loss 9.1077 | Val loss 8.9825\n",
      "Step 50 | Train loss 5.9681 | Val loss 6.1737\n",
      "Step 100 | Train loss 5.1545 | Val loss 5.7972\n",
      "Step 150 | Train loss 4.8863 | Val loss 5.6213\n",
      "Step 200 | Train loss 4.8833 | Val loss 5.4900\n",
      "Step 250 | Train loss 4.4175 | Val loss 5.4318\n",
      "Step 300 | Train loss 4.4837 | Val loss 5.4036\n",
      "Step 350 | Train loss 4.0470 | Val loss 5.3530\n",
      "Step 400 | Train loss 4.0362 | Val loss 5.3476\n",
      "Step 450 | Train loss 4.1814 | Val loss 5.3393\n"
     ]
    }
   ],
   "source": [
    "max_steps = 500  # adjust as needed\n",
    "log_interval = 50\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(10):  # just loop enough until steps reached\n",
    "    for x,y in train_loader:\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B,T,V = logits.size()\n",
    "        loss = F.cross_entropy(logits.view(B*T,V), y.view(B*T))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "            print(f\"Step {step} | Train loss {loss.item():.4f} | Val loss {val_loss:.4f}\")\n",
    "        step += 1\n",
    "    if step >= max_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f287e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the buggy with the top of a corner. The Gargoyles, and the roof, who was so that we do.\" \"Oh, too,\" said the buggy and the earth to go away.\" He, who had no way. As the other that no time I shall we were you when Jim, after of our own big horse, Jim, and Dorothy, for such a little man was a small crack in her face. \"But in a moment her breath--or he has been horrified; \"and are any way to me to this thing it. Also I suppose you will be afraid I had gone. No one of him. Just then you all my dear we can know this he said to the buggy and you are all the little Wizard, I can't you the piglets and I found the Sorcerer, so you with us up in the floor!\" cried Dorothy?\" asked the girl. \"Why, too,\" continued another, but there. The Wizard now, and if I left you?\" asked\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, prompt=\"Once upon a time\", max_new_tokens=200, temperature=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
